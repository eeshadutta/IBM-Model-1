{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from nltk.corpus import comtrans\n",
    "import csv\n",
    "import json\n",
    "import math\n",
    "from nltk.metrics import *\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_corpus_en_hi(source_lang_file, target_lang_file):\n",
    "    sentence_pairs = []\n",
    "    e_file = open(target_lang_file, \"r\")\n",
    "    f_file = open(source_lang_file, \"r\")\n",
    "    e_sentences = e_file.readlines()\n",
    "    f_sentences = f_file.readlines()\n",
    "\n",
    "    for e_s, f_s in zip(e_sentences, f_sentences):\n",
    "        s_pair = {'e': e_s.lower(), 'f': f_s}\n",
    "        sentence_pairs.append(s_pair)\n",
    "\n",
    "    e_file.close()\n",
    "    f_file.close()\n",
    "\n",
    "    return sentence_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_corpus_en_fr(aligned_sentences):\n",
    "    sentence_pairs = []\n",
    "    alignments = []\n",
    "    for aligned_sentence in aligned_sentences:\n",
    "        e_s = \" \".join(aligned_sentence.words)\n",
    "        f_s = \" \".join(aligned_sentence.mots)\n",
    "        s_pair = {'e': e_s.lower(), 'f': f_s.lower()}\n",
    "        sentence_pairs.append(s_pair)\n",
    "        alignments.append(aligned_sentence.alignment)\n",
    "    return sentence_pairs, alignments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vocabulary(sentence_pairs):\n",
    "    e_words_set = set()\n",
    "    f_words_set = set()\n",
    "    for pair in sentence_pairs:\n",
    "        for word in pair['e'].split():\n",
    "            e_words_set.add(word)\n",
    "        for word in pair['f'].split():\n",
    "            f_words_set.add(word)\n",
    "    words = {'e': e_words_set, 'f': f_words_set}\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(sentence_pairs):\n",
    "    words = create_vocabulary(sentence_pairs)\n",
    "    e_words_size = len(words['e'])\n",
    "    f_words_size = len(words['f'])\n",
    "\n",
    "    translation_prob = {}\n",
    "\n",
    "    num_iterations = 20\n",
    "    curr_iteration = 1\n",
    "    while curr_iteration <= num_iterations:\n",
    "        print(\"Iteration number:\", curr_iteration)\n",
    "        count = {}\n",
    "        total = {f_word: 0 for f_word in words['f']}\n",
    "\n",
    "        s_total = {e_word: 0 for e_word in words['e']}\n",
    "        for pair in sentence_pairs:\n",
    "            for e_word in pair['e'].split():\n",
    "                if e_word not in translation_prob:\n",
    "                    translation_prob[e_word] = {}\n",
    "                if e_word not in count:\n",
    "                    count[e_word] = {}\n",
    "                s_total[e_word] = 0\n",
    "                for f_word in pair['f'].split():\n",
    "                    if f_word not in translation_prob[e_word]:\n",
    "                        translation_prob[e_word][f_word] = 1 / f_words_size\n",
    "                    if f_word not in count[e_word]:\n",
    "                        count[e_word][f_word] = 0\n",
    "                    s_total[e_word] += translation_prob[e_word][f_word]\n",
    "\n",
    "            for e_word in pair['e'].split():\n",
    "                for f_word in pair['f'].split():\n",
    "                    count[e_word][f_word] += translation_prob[e_word][f_word] / \\\n",
    "                        s_total[e_word]\n",
    "                    total[f_word] += translation_prob[e_word][f_word] / \\\n",
    "                        s_total[e_word]\n",
    "\n",
    "        for e_word in translation_prob:\n",
    "            for f_word in translation_prob[e_word]:\n",
    "                translation_prob[e_word][f_word] = count[e_word][f_word] / \\\n",
    "                    total[f_word]\n",
    "\n",
    "        curr_iteration += 1\n",
    "\n",
    "    return translation_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alignment_scores(sentence_pairs, translation_prob):\n",
    "    my_alignments = []\n",
    "    for sent in sentence_pairs:\n",
    "        e_words = sent['e'].split()\n",
    "        f_words = sent['f'].split()\n",
    "        sent_alignment = []\n",
    "        for e in e_words:\n",
    "            max_prob = 0\n",
    "            ind = -1\n",
    "            for f in f_words:\n",
    "                if f in translation_prob[e]:\n",
    "                    if translation_prob[e][f] > max_prob:\n",
    "                        max_prob = translation_prob[e][f]\n",
    "                        ind = f_words.index(f)\n",
    "            curr_alignment = (e_words.index(e), ind)\n",
    "            sent_alignment.append(curr_alignment)\n",
    "        my_alignments.append(sent_alignment)\n",
    "    return my_alignments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration number: 1\n",
      "Iteration number: 2\n",
      "Iteration number: 3\n",
      "Iteration number: 4\n",
      "Iteration number: 5\n",
      "Iteration number: 6\n",
      "Iteration number: 7\n",
      "Iteration number: 8\n",
      "Iteration number: 9\n",
      "Iteration number: 10\n",
      "Iteration number: 11\n",
      "Iteration number: 12\n",
      "Iteration number: 13\n",
      "Iteration number: 14\n",
      "Iteration number: 15\n",
      "Iteration number: 16\n",
      "Iteration number: 17\n",
      "Iteration number: 18\n",
      "Iteration number: 19\n",
      "Iteration number: 20\n"
     ]
    }
   ],
   "source": [
    "# English French corpus\n",
    "aligned_sentences = comtrans.aligned_sents('alignment-en-fr.txt')\n",
    "sentence_pairs, alignments = create_corpus_en_fr(aligned_sentences)\n",
    "ind = math.floor(0.8 * len(sentence_pairs))\n",
    "sentence_pairs_train = sentence_pairs[:ind]\n",
    "sentence_pairs_test = sentence_pairs[ind:]\n",
    "translation_prob = train_model(sentence_pairs)\n",
    "\n",
    "outfile = open(\"translation_probablities_en-fr\", \"wb\")\n",
    "pickle.dump(translation_prob, outfile)\n",
    "outfile.close()\n",
    "json_dump = json.dumps(translation_prob)\n",
    "f = open(\"translation_probabilities_en-fr.json\", \"w\")\n",
    "f.write(json_dump)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"translation_probabilities_en-fr.txt\", \"w\")\n",
    "for e in translation_prob:\n",
    "    for f in translation_prob[e]:\n",
    "        if translation_prob[e][f] > 0.3:\n",
    "            file.write(e + \" \" + f + \" : \" + str(translation_prob[e][f]) + \"\\n\")\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# infile = open('translation_probablities_en-fr', 'rb')\n",
    "# translation_prob = pickle.load(infile)\n",
    "# infile.close()\n",
    "file = open(\"translation_probabilities_highest_en-fr.txt\", \"w\")\n",
    "for e in translation_prob:\n",
    "    max_prob = 0\n",
    "    aligned_word = ''\n",
    "    for f in translation_prob[e]:\n",
    "        if translation_prob[e][f] > max_prob:\n",
    "            max_prob = translation_prob[e][f]\n",
    "            aligned_word = f\n",
    "    file.write(e + \" : \" + aligned_word + \" = \" + str(max_prob) + \"\\n\")\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision = 0.6834336688145523\n",
      "Recall = 0.571224106012276\n",
      "Alignment Error Rate = 0.38116104308386844\n"
     ]
    }
   ],
   "source": [
    "my_alignments = alignment_scores(sentence_pairs_test, translation_prob)\n",
    "total_prec = 0\n",
    "total_rec = 0\n",
    "total_aer = 0\n",
    "total_num = 0\n",
    "for (als, my_als) in zip(alignments[ind:], my_alignments):\n",
    "    my_als_set = set()\n",
    "    for al in my_als:\n",
    "        my_als_set.add(al)\n",
    "    als_set = set(als)\n",
    "    total_prec += precision(als_set, my_als_set)\n",
    "    total_rec += recall(als_set, my_als_set)\n",
    "    total_aer += nltk.metrics.alignment_error_rate(als_set, my_als_set)\n",
    "    total_num += 1\n",
    "prec = total_prec / total_num\n",
    "rec = total_rec / total_num\n",
    "aer = total_aer / total_num\n",
    "print(\"Precision =\", prec)\n",
    "print(\"Recall =\", rec)\n",
    "print(\"Alignment Error Rate =\", aer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration number: 1\n",
      "Iteration number: 2\n",
      "Iteration number: 3\n",
      "Iteration number: 4\n",
      "Iteration number: 5\n",
      "Iteration number: 6\n",
      "Iteration number: 7\n",
      "Iteration number: 8\n",
      "Iteration number: 9\n",
      "Iteration number: 10\n",
      "Iteration number: 11\n",
      "Iteration number: 12\n",
      "Iteration number: 13\n",
      "Iteration number: 14\n",
      "Iteration number: 15\n",
      "Iteration number: 16\n",
      "Iteration number: 17\n",
      "Iteration number: 18\n",
      "Iteration number: 19\n",
      "Iteration number: 20\n"
     ]
    }
   ],
   "source": [
    "# English Hindi corpus\n",
    "data_directory = './Data'\n",
    "source_lang_file = './Data/train.hi'\n",
    "target_lang_file = './Data/train.en'\n",
    "sentence_pairs = create_corpus_en_hi(source_lang_file, target_lang_file)\n",
    "translation_prob = train_model(sentence_pairs)\n",
    "\n",
    "outfile = open(\"translation_probablities_en-hi\", \"wb\")\n",
    "pickle.dump(translation_prob, outfile)\n",
    "outfile.close()\n",
    "json_dump = json.dumps(translation_prob)\n",
    "f = open(\"translation_probabilities_en-hi.json\", \"w\")\n",
    "f.write(json_dump)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"translation_probabilities_en-hi.txt\", \"w\")\n",
    "for e in translation_prob:\n",
    "    for f in translation_prob[e]:\n",
    "        if translation_prob[e][f] > 0.3:\n",
    "            file.write(e + \" \" + f + \" : \" + str(translation_prob[e][f]) + \"\\n\")\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# infile = open('translation_probablities_en-hi', 'rb')\n",
    "# translation_prob = pickle.load(infile)\n",
    "# infile.close()\n",
    "file = open(\"translation_probabilities_highest_en-hi.txt\", \"w\")\n",
    "for e in translation_prob:\n",
    "    max_prob = 0\n",
    "    aligned_word = ''\n",
    "    for f in translation_prob[e]:\n",
    "        if translation_prob[e][f] > max_prob:\n",
    "            max_prob = translation_prob[e][f]\n",
    "            aligned_word = f\n",
    "    file.write(e + \" : \" + aligned_word + \" = \" + str(max_prob) + \"\\n\")\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
